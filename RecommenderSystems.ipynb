{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1><center>Project 4<BR><BR>\n",
    "Recommender Systems</center></H1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Task 1: Algorithmic Bias\n",
    "</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P>Read <a href=\"https://cacm.acm.org/magazines/2016/10/207759-battling-algorithmic-bias/abstract\">this article</a> (also available <a href=\"https://drive.google.com/file/d/1bRKH--BoCAQw7ZyK8xmBGGZR2lyPlRaN/view?usp=sharing\">here</a>) about bias in algorithms.</P>\n",
    "\n",
    "<P>Please address (minimum 200 words) the following questions in the space below. As the article describes, \"common wisdom among programmers is to develop a pure algorithm that does not incorporate protected attributes into the model,\" where protected classes may include aspects such as race, gender, age, sexual orientation, and disability status. As a result, the algorithms can be bias. How might algorithms be evaluated for bias? Are there machine learning applications where you would sacrifice accuracy in order to reduce bias? Are there machine learning applications where you would allow bias in order to increase accuracy?</P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect bias in an algorithm, you could look for patterns in the results and determine how those patterns were created. Since biases are inherently harmful and prejudiced, it is hard to imagine a case where we might allow bias in an algorithm. This would have to be a case where we want an algorithm to make an unfair judgment. I could see how this may be useful in some sort of academic study. But only if that study needs these biases to be present in order to draw conclusions. To prevent bias in algorithms, it may be good to try and eliminate bias at the source. So if the algorithm is using data from a sample, we should ensure that that data was collected properly and fairly. We can also evaluate the results of the algorithm to ensure that if the features that could lead to bias (race, gender, sexual-orientation, etc.) were changed, the algorithmâ€™s output would remain the same. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Task 2: Pipeline for Classification\n",
    "</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P>As broader context for this project, little or no starter code is provided for the tasks in the project. One of the goals of this lack of starter code is to give you an opportunity to practice developing meaningful computational artifacts mostly or entirely from scratch.</P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P>To start, let's make a simple pipeline that integrates in one function some of the aspects of supervised classification.</P>\n",
    "\n",
    "<P>Create a function named <code>pipeline</code> that takes three arguments:</P>\n",
    "<ol>\n",
    "    <li>The name of a comma separated values (CSV) file. Assume the file has a header row and all subsequent rows contain numerical data. The last column in the file corresponds to classification labels.</li>\n",
    "    <li>A Boolean indicating whether feature scaling should be performed on the data in the file.</li>\n",
    "    <li>A supervised classification model, such as a random forest classifier, a <em>k</em> nearest neighbors classifier, a logistic regression classifier, or a support vector machine.</li>\n",
    "</ol>\n",
    "\n",
    "<P>Your <code>pipeline</code> function should:</P>\n",
    "<ol>\n",
    "    <li>read in the specified file</li>\n",
    "    <li>extract the feature vectors (e.g., <em>X</em>) and labels (e.g., <em>y</em>)</li>\n",
    "    <li>split the data into training (80%) and testing (20%) (with <code>random_state=0</code>)</li>\n",
    "    <li>perform feature scaling <em>if</em> the specified Boolean argument is <code>True</code></li>\n",
    "    <li>train the classification model on the training data</li>\n",
    "    <li>print out the accuracy of the classification model on the testing data</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The *pipeline* function executes some of the aspects of supervised\n",
    "# classification on a set of data.\n",
    "# The function takes three arguments: the name of a CSV file with a header line and\n",
    "# whose last column contains labels, a Boolean indicating if feature scaling\n",
    "# should be performed on the data in the file, and a supervised classification model.\n",
    "# The function reads in data from the file, splits the data into training (80%)\n",
    "# and testing (20%), performs feature scaling if specified by the Boolean argument,\n",
    "# trains the model, and prints out the model's accuracy on the testing data.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors #knn\n",
    "from sklearn import linear_model #perceptron and logistic regression\n",
    "from sklearn import ensemble #random forest\n",
    "from sklearn import svm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "learners = {'Perceptron': linear_model.Perceptron(max_iter=10),\n",
    "            'RandomForest': ensemble.RandomForestClassifier(),\n",
    "            'kNN': neighbors.KNeighborsClassifier(), \n",
    "            'logistic': linear_model.LogisticRegression(random_state=0),\n",
    "            'SVM':svm.SVC()\n",
    "           }\n",
    "\n",
    "#for classifierName in learners:\n",
    "    #learners[classifierName].fit(X_train, y_train)\n",
    "    #print('Accuracy of ' + classifierName + ':\\t' + str(learners[classifierName].score(X_test, y_test)))\n",
    "\n",
    "#decision tree\n",
    "#DecisionTreeClassifier = tree.DecisionTreeClassifier() \n",
    "#RandomForestClassifier = ensemble.RandomForestClassifier()\n",
    "\n",
    "#knn\n",
    "#KNeighborsClassifier = neighbors.KNeighborsClassifier()\n",
    "\n",
    "#perceptron\n",
    "#Perceptron = linear_model.Perceptron(max_iter=10, random_state=0)\n",
    "\n",
    "#logistic\n",
    "#LogisticRegression = linear_model.LogisticRegression(random_state=0)\n",
    "\n",
    "#svm \n",
    "#SupportVectorMachine = svm.SVC()\n",
    "\n",
    "def pipeline(csv_file, shouldScale, model):\n",
    "        \n",
    "    #model = KNeighborsClassifier()\n",
    "    \n",
    "    DATA = np.loadtxt(csv_file, delimiter=',', skiprows=1)\n",
    "    X = DATA[:, :-1]\n",
    "    y = DATA[:, -1] \n",
    "      \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "    \n",
    "    if(shouldScale):\n",
    "        scaler = StandardScaler()\n",
    "        X_trainScaled = scaler.fit_transform(X_train)\n",
    "        learners[model].fit(X_trainScaled, y_train)\n",
    "        result = learners[model].score(X_test, y_test)\n",
    "        #result2 = model.score(X_train, y_train)\n",
    "    else:\n",
    "        learners[model].fit(X_train, y_train)\n",
    "        result = learners[model].score(X_test, y_test)\n",
    "\n",
    "    print('score on ' + str(csv_file) + ' data using ' + str(model) + ':\\t' + str(result))\n",
    "    #print('score on train data using ' + str(model) + ':\\t' + str(result2))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P>With the <code>pipeline</code> function above, you have a mechanism for quickly invoking several aspects of supervised classification. Below, you should experiment with executing your <code>pipeline</code> function on the following four datasets:</P>\n",
    "<ol>\n",
    "    <li>The file <code>admission.csv</code> contains GPA and GMAT score data on people who have/haven't been <a href=\"https://www.kaggle.com/willianleite/mba-admission\">admitted to an MBA program</a> (graduate school in business). How well can you predict who will be admitted based on their GPA and their GMAT standardized test score?</li>\n",
    "    <li>The file <code>dementia.csv</code> contains MRI scans and other health data on people who do/don't have <a href=\"https://www.kaggle.com/shashwatwork/dementia-prediction-dataset\">dementia</a>. How well can you predict who has dementia based on this health data?</li>\n",
    "    <li>The file <code>star.csv</code> contains chromaticity and spectral data for different stars in the galaxy. In this <a href=\"https://www.kaggle.com/vinesmsuic/star-categorization-giants-and-dwarfs\">stellar classification problem</a>, how well can you determine which stars are \"giants\" and which are \"dwarfs\".</li>\n",
    "    <li>The file <code>airlineDelays.csv</code> contains data on airplane flights and whether or not the flight was delayed. Data include information on the airline, the departing airport of the flight, the weather, the number of seats on the flight, the number of flight attendants, the age of the plane, etc. How well can you predict <a href=\"https://www.kaggle.com/threnjen/2019-airline-delays-and-cancellations\">flight delays</a>?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on admission.csv data using RandomForest:\t1.0\n",
      "score on dementia.csv data using SVM:\t0.49333333333333335\n",
      "score on star.csv data using kNN:\t0.5015\n",
      "score on airlineDelays.csv data using logistic:\t0.494\n"
     ]
    }
   ],
   "source": [
    "# Experiments executing *pipeline* on four different data sets\n",
    "\n",
    "pipeline('admission.csv',False, 'RandomForest')\n",
    "#pipeline('admission.csv',False, RandomForestClassifier)\n",
    "pipeline('dementia.csv',True, 'SVM')\n",
    "pipeline('star.csv',True, 'kNN')\n",
    "pipeline('airlineDelays.csv',True, 'logistic')\n",
    "\n",
    "#bad results? Perceptron and Logistic have exact same score??? weird\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>For the <code>admission</code> data, what is the testing accuracy <code>without</code> feature scaling and using a <code>Random Forest</code> classifier (with <code>random_state=0</code>)?<u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>For the <code>dementia</code> data, what is the testing accuracy <code>with</code> feature scaling and using a <code>Support Vector Machine</code> (with <code>random_state=0</code>)?<u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>For the <code>star</code> data, what is the testing accuracy <code>with</code> feature scaling and using a <code><em>k</em> Nearest Neighbors</code> classifier?<u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>For the <code>airline delays</code> data, what is the testing accuracy <code>with</code> feature scaling and using a <code>Logistic Regression</code> classifier (with <code>random_state=0</code>)?<u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Task 3: Sentiment Analysis of Movie Reviews\n",
    "</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P>Remember in Exercise 3 where you performed sentiment analysis of Twitter data? If not, reviewing it will be very helpful here :). Here you will perform sentiment analysis of <a href=\"https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\">movie reviews</a>, i.e., you will determine whether a movie review is negative or positive based on the text of the review.</P>\n",
    "<P>The file <code>movie_reviews.txt</code> contains 50,000 lines. Each line contains a review of a movie followed by a label, either \"@@@negative\" or \"@@@positive\", indicating if the review is negative or positive. Your task is to:</P>\n",
    "<ol>\n",
    "    <li>read in the data from the file</li>\n",
    "    <li>randomly shuffle the data, while making sure each label stays with its corresponding review</li>\n",
    "    <li>store the data as a corpus and as labels</li>\n",
    "    <li>split the data into training (80%) and testing (20%)</li>\n",
    "    <li>use the Bag of Words approach (tf-idf) to vectorize the data (you should use unigrams rather than bigrams here)</li>\n",
    "    <li>use a logistic regression classifier to predict the sentiment of reviews</li>\n",
    "    <li>based on the logistic regression model weights, determine the 10 lowest weighted words (indicative of negative sentiment reviews) and the 10 highest weighted words (indicative of positive sentiment reviews)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in movie review data from file.\n",
    "# Randomly shuffle data, making sure each label stays with its corresponding review.\n",
    "# Store data as corpus and labels.\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "\n",
    "def readMovieFile(txt_fileName):\n",
    "    DATA = []\n",
    "    txt_file = open(txt_fileName, encoding=\"utf-8\")\n",
    "    reviews = txt_file.read().split('\\n')\n",
    "\n",
    "    for review in reviews: \n",
    "        reviewSplit = review.split('@@@') #review is an array with two strings: one with the review message and the other with the word positive/negative\n",
    "        justReview = reviewSplit[0]\n",
    "        justSentiment = reviewSplit[1]\n",
    "        DATA.append([justReview, justSentiment])\n",
    "        \n",
    "    txt_file.close()\n",
    "\n",
    "    random.shuffle(DATA)\n",
    "    return [row[0] for row in DATA], [row[1] for row in DATA]\n",
    "\n",
    "corpus, labels = readMovieFile('movie_reviews.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "40000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TEST_SIZE = 0.2  # 20% testing data and 80% training data\n",
    "\n",
    "separator = int((1.0 - TEST_SIZE)*len(corpus))\n",
    "corpus_train = corpus[:separator]\n",
    "labels_train = labels[:separator]\n",
    "corpus_test = corpus[separator:]\n",
    "labels_test = labels[separator:]\n",
    "print(len(corpus_train))\n",
    "print(len(labels_train))\n",
    "print(len(corpus_test))\n",
    "print(len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 93110)\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "# Text feature extraction (using Bag of Words approach with unigrams) for training data\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "X_train = vectorizer.fit_transform(corpus_train)\n",
    "y_train = np.array(labels_train)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 93110)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# Text feature extraction (using Bag of Words approach with unigrams) for testing data\n",
    "\n",
    "X_test = vectorizer.transform(corpus_test)\n",
    "y_test = np.array(labels_test)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of Logistic Regression on review sentiment: 0.8988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopezhu/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Use logistic regression to predict sentiment of reviews (with random_state=0)\n",
    "from sklearn import linear_model\n",
    "\n",
    "logistic = linear_model.LogisticRegression(random_state=0)\n",
    "logistic.fit(X_train, y_train)\n",
    "score = logistic.score(X_test, y_test)\n",
    "\n",
    "print(\"Score of Logistic Regression on review sentiment: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowest weighted words (indicative of negative sentiment reviews)\n",
      "worst\n",
      "bad\n",
      "waste\n",
      "awful\n",
      "boring\n",
      "terrible\n",
      "poor\n",
      "nothing\n",
      "horrible\n",
      "worse\n",
      "\n",
      "Highest weighted words (indicative of positive sentiment reviews)\n",
      "great\n",
      "excellent\n",
      "best\n",
      "perfect\n",
      "wonderful\n",
      "today\n",
      "loved\n",
      "amazing\n",
      "hilarious\n",
      "fun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Get Logistic Regression weights\n",
    "\n",
    "learner = linear_model.LogisticRegression(random_state=0)\n",
    "learner.fit(X_train, y_train)\n",
    "weights = learner.coef_ \n",
    "sorted_weights = np.argsort(weights)  # Sort\n",
    "features = vectorizer.get_feature_names()  # Get the features\n",
    "print('\\nLowest weighted words (indicative of negative sentiment reviews)')\n",
    "for i in range(10): print(features[sorted_weights[0,i]])\n",
    "print('\\nHighest weighted words (indicative of positive sentiment reviews)')\n",
    "for i in range(10): print(features[sorted_weights[0,len(sorted_weights[0])-i-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>What is the testing accuracy of your logistic regression model at determining the sentiment of movie reviews?<u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "89.88%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<P><font color=\"maroon\"><u>For your logistic regression classifier, what are the 10 lowest weighted words, indicative of negative reviews?<u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "worst\n",
    "bad\n",
    "waste\n",
    "awful\n",
    "boring\n",
    "terrible\n",
    "poor\n",
    "nothing\n",
    "horrible\n",
    "worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>For your logistic regression classifier, what are the 10 highest weighted words, indicative of positive reviews?<u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "great\n",
    "excellent\n",
    "best\n",
    "perfect\n",
    "wonderful\n",
    "today\n",
    "loved\n",
    "amazing\n",
    "hilarious\n",
    "fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Task 4: Detecting Fake News\n",
    "</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P>Can you use machine learning to distinguish <a href=\"https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset\">fake news stories from true news stories</a>? You are provided two files, each containing over 20,000 lines, where each line corresponds to a news story. The file <code>Fake_News.txt</code> contains fake news stories. The file <code>True_News.txt</code> contains true news stories. Using the Bag of Words (tf-idf) approach, your task is to distinguish fake news stories from true news stories. We are not providing much guidance here (and no starter code) so that you can think about how best to tackle this problem largely from scratch.</P>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize  returns a list of words(tokens) which was created from the input\n",
    "def tokenize(text):\n",
    "    return text.replace('!', ' ').replace('.', ' ').replace('?', ' ').replace(',', ' ').lower().split()\n",
    "\n",
    "# Test the tokenize function\n",
    "#tokenize('What is zero divided by zero?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. \n",
    "# Rather than put all your code in this one cell, create multiple new cells as needed.\n",
    "\n",
    "#read in file\n",
    "import random\n",
    "random.seed(42)\n",
    "def readNewsData(fake_filename, true_filename):\n",
    "    DATA = []\n",
    "\n",
    "    # Read in fake messages from file\n",
    "    fake_file = open(fake_filename, encoding=\"utf-8\") #why do we need this encoding line???\n",
    "    fakeNewsStories = fake_file.read().split('\\n')\n",
    "    for fakeStory in fakeNewsStories: \n",
    "        DATA.append([fakeStory, 0])\n",
    "    fake_file.close()\n",
    "    \n",
    "    # Read in true messages from file\n",
    "    true_file = open(true_filename,  encoding=\"utf-8\")\n",
    "    trueNewsStories= true_file.read().split('\\n')\n",
    "    for trueStory in trueNewsStories: \n",
    "        DATA.append([trueStory, 1])\n",
    "    true_file.close()\n",
    "\n",
    "    random.shuffle(DATA)  # Shuffle\n",
    "    return [row[0] for row in DATA], [row[1] for row in DATA]\n",
    "\n",
    "corpus, labels = readNewsData('Fake_News.txt', 'True_News.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into training and testing data\n",
    "TEST_SIZE = 0.2  # 20% testing data and 80% training data\n",
    "\n",
    "separator = int((1.0 - TEST_SIZE)*len(corpus))\n",
    "corpus_train = corpus[:separator]\n",
    "labels_train = labels[:separator]\n",
    "\n",
    "corpus_test = corpus[separator:]\n",
    "labels_test = labels[separator:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35407, 2541612)\n",
      "(35407,)\n"
     ]
    }
   ],
   "source": [
    "#vectorizing the training data, i.e., use the Bag of Words approach to count how many times each word appears in each news story\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))  # Use individual words as tokens\n",
    "X_train = vectorizer.fit_transform(corpus_train)\n",
    "y_train = np.array(labels_train)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8852, 2541612)\n",
      "(8852,)\n"
     ]
    }
   ],
   "source": [
    "#now the testing data\n",
    "X_test = vectorizer.transform(corpus_test)\n",
    "y_test = np.array(labels_test)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Perceptron:\t0.9915273384545865\n",
      "Accuracy of kNN:\t0.8846588341617714\n",
      "Accuracy of logistic:\t0.9826028016267511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopezhu/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM:\t0.7007455942159964\n"
     ]
    }
   ],
   "source": [
    "# Use different classifiers to predict if a news story is real or fake\n",
    "from sklearn import linear_model\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "\"\"\"\n",
    "c = [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "gamma = [0.1,1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "for val in c:\n",
    "    for value in gamma:\n",
    "        vector = svm.SVC(C=val, gamma = value, max_iter = 10)\n",
    "        vector.fit(X_train, y_train)\n",
    "        score = vector.score(X_test, y_test)\n",
    "        print(\"accuracy with c:\" + str(val) + \"and gamma:\" + str(value) + \"is:\" + str(score))\n",
    "\"\"\"\n",
    "learners = {'Perceptron': linear_model.Perceptron(max_iter=10),\n",
    "            #'RandomForest': ensemble.RandomForestClassifier(),\n",
    "            'kNN': neighbors.KNeighborsClassifier(), \n",
    "            'logistic': linear_model.LogisticRegression(random_state=0),\n",
    "            'SVM':svm.SVC(C=10.0, gamma=1.0, max_iter = 20)\n",
    "           }\n",
    "for classifierName in learners:\n",
    "    learners[classifierName].fit(X_train, y_train)\n",
    "    print('Accuracy of ' + classifierName + ':\\t' + str(learners[classifierName].score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hopezhu/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowest weighted words (indicative of fake news stories)\n",
      "via\n",
      "read more\n",
      "president trump\n",
      "sen\n",
      "read\n",
      "this\n",
      "mr\n",
      "featured image\n",
      "com\n",
      "featured\n",
      "\n",
      "Highest weighted words (indicative of true news stories)\n",
      "said\n",
      "on\n",
      "said on\n",
      "president donald\n",
      "said in\n",
      "on monday\n",
      "president barack\n",
      "republican\n",
      "showed\n",
      "reported on\n"
     ]
    }
   ],
   "source": [
    "# Get Perceptron weights to figure out what features were weighted the highest and lowest for classification. \n",
    "learner = linear_model.Perceptron(max_iter=10)\n",
    "learner.fit(X_train, y_train)\n",
    "\n",
    "weights = learner.coef_  # Get the learned Perceptron weights\n",
    "sorted_weights = np.argsort(weights)  # Sort\n",
    "features = vectorizer.get_feature_names()  # Get the features\n",
    "\n",
    "print('\\nLowest weighted words (indicative of fake news stories)')\n",
    "for i in range(10): print(features[sorted_weights[0,i]])\n",
    "print('\\nHighest weighted words (indicative of true news stories)')\n",
    "for i in range(10): print(features[sorted_weights[0,len(sorted_weights[0])-i-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>What is the testing accuracy of your machine learning method in determining whether a news story is fake?<u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "99.15%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>What are 10 words highly associated with true news stories?<u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "said\n",
    "on\n",
    "said on\n",
    "president donald\n",
    "said in\n",
    "on monday\n",
    "president barack\n",
    "republican\n",
    "showed\n",
    "reported on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>What are 10 words highly associated with fake news stories?<u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "via\n",
    "read more\n",
    "president trump\n",
    "sen\n",
    "read\n",
    "this\n",
    "mr\n",
    "featured image\n",
    "com\n",
    "featured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Submitting your work\n",
    "</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>Please indicate your name and the names of any partner that worked with you on this project:</u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name(s): Kate MacVicar and Hope Zhu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>Please indicate anyone else that you collaborated with in the process of doing the project:</u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborators: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>When working on this project, approximately how many hours did you spend on each of (1) Task 1, (2) Task 2, (3) Task 3, (4) Task 4, (5) Task 5, and (6) Total?</u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hours on Task 1: \n",
    "Hours on Task 2: \n",
    "Hours on Task 3: \n",
    "Hours on Task 4: \n",
    "Hours on Task 5: \n",
    "Total hours: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>When working on this project, did you abide by the <a href=\"https://www.wellesley.edu/studentlife/aboutus/honor\">Honor Code</a> and is all of the work that you are submitting your own and/or your partner's?</u></font></P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abide by Honor Code: yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P><font color=\"maroon\"><u>To submit this project, please upload your <code>Project4.ipynb</code> file to the <code>Project4</code> folder that the instructor created and shared with you in your Google drive.</u></font></P>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
